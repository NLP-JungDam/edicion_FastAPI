import os
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware

load_dotenv()

# âœ… OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
if not api_key:
    raise ValueError("ğŸš¨ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
os.environ["OPENAI_API_KEY"] = api_key

host = os.getenv("SERVER_HOST")
port = os.getenv("SERVER_PORT")

# âœ… 2ê°œì˜ LLM ëª¨ë¸ ì„¤ì •
model_1 = ChatOpenAI(temperature=0, model_name="gpt-4o-mini-2024-07-18")  # ì í•©ë„ í‰ê°€
model_2 = ChatOpenAI(temperature=0, model_name="gpt-4o-mini-2024-07-18")  # ì²¨ì‚­ or ê³µë¶€ë²• ì œê³µ
model_3 = ChatOpenAI(temperature=0, model_name="gpt-4o-mini-2024-07-18")  # ì¸ì¬ìœ í˜• íŒë‹¨ ëª¨ë¸

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt_1 = PromptTemplate(
    input_variables=["lorem", "jobObjective"],
    template="""
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ìê¸°ì†Œê°œì„œ ë‚´ìš©ì´ {jobObjective}ì— ì–¼ë§ˆë‚˜ ì í•©í•œì§€ 0~100% ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•˜ê³ ,
    ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´.
    
    ì í•©ë„ ì ìˆ˜: <ìˆ«ì>%
    
    í‰ê°€í•  ìê¸°ì†Œê°œì„œ:
    {lorem}
    """
)

prompt_2_resume = PromptTemplate(
    input_variables=["lorem", "jobObjective"],
    template="""
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œë¥¼ {jobObjective}ì§ë¬´ì— ë§ê²Œ ìµœì í™”í•´ì„œ ìˆ˜ì •í•´ì£¼ê³ , 400ì ì´ìƒ ì‘ì„±í•´ì¤˜.
    ì‚¬ìš©ìê°€ ì–´ë–¤ ë§íˆ¬ë¡œ ì…ë ¥ì„ í•´ë„ ë„ˆëŠ” ìê¸°ì†Œê°œì„œ ë§íˆ¬ì— ë§ê²Œ ~ì…ë‹ˆë‹¤ì²˜ëŸ¼ ì‘ì„±í•´ì£¼ê³  ë˜ë„ë¡ì´ë©´ STAR ê¸°ë²•ì„ ì‚¬ìš©í•´ì„œ ì‘ì„±í•´ì£¼ë©´ ì¢‹ê² ì–´
    
    ì›ë³¸:
    {lorem}
    
    ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œ:
    """
)

prompt_2_study = PromptTemplate(
    input_variables=["jobObjective"],
    template="""
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œê°€ {jobObjective}ì— ì í•©í•˜ì§€ ì•ŠìŒ.
    ë”°ë¼ì„œ {jobObjective}ì— ë§ëŠ” ì‹¤ë ¥ì„ í‚¤ìš°ê¸° ìœ„í•œ ê³µë¶€ë²•ê³¼ ë°©í–¥ì„ ì œê³µí•´ì¤˜.
    
    ì¶”ì²œ ê³µë¶€ë²•:
    """
)

# âœ… ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ë¡œ ë³€ê²½
async def process_pipeline(lorem, jobObjective):
    # 1ï¸âƒ£ í¬ë§ ì§ë¬´ ì í•©ë„ í‰ê°€ (ë¹„ë™ê¸° í˜¸ì¶œ)
    response_1_obj = await (prompt_1 | model_1).ainvoke({"lorem": lorem, "jobObjective": jobObjective})
    response_1 = response_1_obj.content
    response_1 = response_1.replace("ì í•©ë„ ì ìˆ˜: ", "").strip("%")
    print(f"ğŸ”¹ ì í•©ë„ í‰ê°€ ê²°ê³¼: {response_1}")

    # 2ï¸âƒ£ ì í•©ë„ê°€ 75% ì´ìƒì´ë©´ ìê¸°ì†Œê°œì„œ ì²¨ì‚­, ì•„ë‹ˆë©´ ê³µë¶€ë²• ì¶”ì²œ
    if int(response_1) >= 75:
        response_2_obj = await (prompt_2_resume | model_2).ainvoke({"lorem": lorem, "jobObjective": jobObjective})
        response_2 = response_2_obj.content
        return {"ability": response_1, "resume": response_2, "lorem": lorem}
    else:
        response_2_obj = await (prompt_2_study | model_2).ainvoke({"jobObjective": jobObjective})
        response_2 = response_2_obj.content
        return {"ability": response_1, "study": response_2, "lorem": lorem}

# âœ… FastAPI ì„¤ì •
app = FastAPI()

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ (ì™¸ë¶€ ìš”ì²­ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í•„ìš”ì— ë”°ë¼ íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš© ê°€ëŠ¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ ëª¨ë¸ ì •ì˜
class ResumeRequest(BaseModel):
    lorem: str
    jobObjective: str

# ì—”ë“œí¬ì¸íŠ¸: ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ì„ ì–¸í•˜ì—¬ await process_pipeline ì‚¬ìš©
@app.post("/user/validate_resume")
async def validate_resume(request: ResumeRequest):
    print("ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì—°ê²°ëìŠµë‹ˆë‹¤.")
    lorem = request.lorem
    jobObjective = request.jobObjective 
    return await process_pipeline(lorem, jobObjective)

# âœ… ì„œë²„ ì‹¤í–‰ (ë‹¤ë¥¸ ì»´í“¨í„°ì—ì„œ ì ‘ì† ê°€ëŠ¥í•˜ë„ë¡ hostì™€ port ì§€ì •)
if __name__ == "__main__":
    uvicorn.run("main:app", host=host, port=port, reload=True)
