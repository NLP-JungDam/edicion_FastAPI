import os
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware

load_dotenv()

# âœ… OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
if not api_key:
    raise ValueError("ğŸš¨ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
os.environ["OPENAI_API_KEY"] = api_key

host = os.getenv("SERVER_HOST")
port = int(os.getenv("SERVER_PORT"))

# âœ… 2ê°œì˜ LLM ëª¨ë¸ ì„¤ì •
model_1 = ChatOpenAI(temperature=0, model_name="gpt-4o-mini-2024-07-18")  # ì í•©ë„ í‰ê°€
model_2 = ChatOpenAI(temperature=0, model_name="gpt-4o-mini-2024-07-18")  # ì²¨ì‚­ or ê³µë¶€ë²• ì œê³µ
model_3 = ChatOpenAI(temperature=0, model_name="gpt-4o-mini-2024-07-18")  # ì¸ì¬ìœ í˜• íŒë‹¨ ëª¨ë¸
model_4 = ChatOpenAI(temperature=0, model_name="gpt-4o-mini-2024-07-18")  # ê¸°ì—…ê³¼ 
 
# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt_1 = PromptTemplate(
    input_variables=["lorem", "jobObjective"],
    template="""
    ë‹¹ì‹ ì€ ì•„ë˜ì˜ 10ê°œ ì§ì—… ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´, ì£¼ì–´ì§„ ìê¸°ì†Œê°œì„œê°€ ì–¼ë§ˆë‚˜ ì í•©í•œì§€ ê°ê° 0~100% ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” í‰ê°€ìì…ë‹ˆë‹¤.

    ì§ì—… ì¹´í…Œê³ ë¦¬:
    1) ì„œë¹„ìŠ¤ì—…
    2) ì œì¡°Â·í™”í•™
    3) ITÂ·ì›¹Â·í†µì‹ 
    4) ì€í–‰Â·ê¸ˆìœµì—…
    5) ë¯¸ë””ì–´Â·ë””ìì¸
    6) êµìœ¡ì—…
    7) ì˜ë£ŒÂ·ì œì•½Â·ë³µì§€
    8) íŒë§¤Â·ìœ í†µ
    9) ê±´ì„¤ì—…
    10) ê¸°ê´€Â·í˜‘íšŒ

    ì‚¬ìš©ìê°€ ì„ íƒí•œ ì§ì—… ëª©í‘œ: {jobObjective}

    í‰ê°€ ê¸°ì¤€:
    1. ë¨¼ì €, ìê¸°ì†Œê°œì„œê°€ {jobObjective} ì§ì—…ì— ì–¼ë§ˆ  ë‚˜ ì í•©í•œì§€ 0~100% ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”.
    2. ê·¸ë‹¤ìŒ, ë‚˜ë¨¸ì§€ 9ê°œ ì§ì—… ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ì„œë„ ê°ê° 0~100% ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”.
    3. ë§ˆì§€ë§‰ìœ¼ë¡œ, ê·¸ 9ê°œ ì§ì—… ì¹´í…Œê³ ë¦¬ ì¤‘ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ìƒìœ„ 2ê°œ ì¹´í…Œê³ ë¦¬ ì´ë¦„ê³¼ ì ìˆ˜ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
    4. ì•„ë˜ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ì •í™•íˆ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í•´ì„¤ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

    ì¶œë ¥ í˜•ì‹(ì˜ˆì‹œ):
    {jobObjective}: 85%
    ì„œë¹„ìŠ¤ì—…: 78%
    ITÂ·ì›¹Â·í†µì‹ : 72%

    í‰ê°€í•  ìê¸°ì†Œê°œì„œ:
    {lorem}
    """
)

prompt_2_resume = PromptTemplate(
    input_variables=["lorem", "jobObjective"],
    template="""
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œë¥¼ {jobObjective}ì§ë¬´ì— ë§ê²Œ ìµœì í™”í•´ì„œ ìˆ˜ì •í•´ì£¼ê³ , 400ì ì´ìƒ ì‘ì„±í•´ì¤˜.
    ì‚¬ìš©ìê°€ ì–´ë–¤ ë§íˆ¬ë¡œ ì…ë ¥ì„ í•´ë„ ë„ˆëŠ” ìê¸°ì†Œê°œì„œ ë§íˆ¬ì— ë§ê²Œ ~ì…ë‹ˆë‹¤ì²˜ëŸ¼ ì‘ì„±í•´ì£¼ê³  ë˜ë„ë¡ì´ë©´ STAR ê¸°ë²•ì„ ì‚¬ìš©í•´ì„œ ì‘ì„±í•´ì£¼ë©´ ì¢‹ê² ì–´
    
    ì›ë³¸:
    {lorem}
    
    ìˆ˜ì •ëœ ìê¸°ì†Œê°œì„œ:
    """
)

prompt_2_study = PromptTemplate(
    input_variables=["jobObjective"],
    template="""
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œê°€ {jobObjective}ì— ì í•©í•˜ì§€ ì•ŠìŒ.
    ë”°ë¼ì„œ {jobObjective}ì— ë§ëŠ” ì‹¤ë ¥ì„ í‚¤ìš°ê¸° ìœ„í•œ ê³µë¶€ë²•ê³¼ ë°©í–¥ì„ ì œê³µí•´ì¤˜.
    
    ì¶”ì²œ ê³µë¶€ë²•:
    """
)

prompt_3 = PromptTemplate(
    input_variables=["resume"],
    template="""
    ì…ë ¥í•œ ìê¸°ì†Œê°œì„œê°€ ì–´ë–¤ ì¸ì¬ ìœ í˜•ì¸ì§€ íŒë‹¨í•´ì¤˜.
    ì±…ì„ì˜ì‹í˜•, ë„ì „ì •ì‹ í˜•, ì†Œí†µí˜‘ë ¥í˜•, ì°½ì˜ì„±í˜• 4ê°€ì§€ ì¤‘ì— í•˜ë‚˜ë¡œ íŒë‹¨í•´ì£¼ë©´ ë¼.
    ì±…ì„ì˜ì‹í˜•, ë„ì „ì •ì‹ í˜•, ì†Œí†µí˜‘ë ¥í˜•, ì°½ì˜ì„±í˜• ì¤‘ í•˜ë‚˜ë¡œ ì¶œë ¥í•´ì£¼ë©´ ë¼
    
    ìê¸°ì†Œê°œì„œ:
    {resume}
    
    ì¸ì¬ ìœ í˜•:
    """
)

prompt_4 = PromptTemplate(
    input_variables=["lorem", "preferred"],
    template="""
    ë„ˆê°€ ê³ ìš©ì£¼ì˜ ì…ì¥ì—ì„œ ê¸°ì—…ì˜ ì¸ì¬ìƒì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ê³¼ ìê¸°ì†Œê°œì„œì˜ ì§ë¬´ì ì¸ ìœ ì‚¬ë„ë¥¼ íŒë‹¨í•´ì•¼í•´.
    0~100% ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•˜ë©´ ë¼.
    ìˆ«ì%ë§Œ ì¶œë ¥í•´ì¤˜
    ê·¸ë¦¬ê³  ê°™ì€ ìê¸°ì†Œê°œì„œì™€ ê¸°ì—… ì¸ì¬ìƒì„ ì…ë ¥í•˜ê³  ì‹¤í–‰ì‹œí‚¤ë©´ ë§¤ë²ˆ ë‹¤ë¥¸ ì¶œë ¥ê°’ì´ ë‚˜ì˜¤ëŠ”ë° ê·¸ëŸ¬ì§€ ì•Šê²Œ ì¼ê´€ì ì¸ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ì¤˜
    
    ìê¸°ì†Œê°œì„œ :
    {lorem}
    
    ê¸°ì—…ì˜ ì¸ì¬ìƒ :
    {preferred}
    
    ì¼ì¹˜ë„ ì ìˆ˜: <ìˆ«ì>%
    """
)

async def process_pipeline(lorem, jobObjective):
    response_1_obj = await (prompt_1 | model_1).ainvoke({"lorem": lorem, "jobObjective": jobObjective})
    response_1_text = response_1_obj.content
    
    scores = {}
    for line in response_1_text.splitlines():
        line = line.strip()
        if not line or ":" not in line:
            continue
        category, score_part = line.split(":", 1)
        score_str = score_part.strip().replace("%", "")
        try:
            score = int(score_str)
            scores[category.strip()] = score
        except ValueError:
            continue

    job_score = scores.get(jobObjective, 0)
    
    total_score = { jobObjective: job_score }
    
    other_scores = {cat: sc for cat, sc in scores.items() if cat != jobObjective}
    top2 = sorted(other_scores.items(), key=lambda x: x[1], reverse=True)[:2]
    for cat, sc in top2:
        total_score[cat] = sc
        
    if job_score >= 75:
        response_2_obj = await (prompt_2_resume | model_2).ainvoke({"lorem": lorem, "jobObjective": jobObjective})
        response_2 = response_2_obj.content
        return {"ability": response_1_text, "resume": response_2, "lorem": lorem, "total_score": total_score}
    
    else:
        response_2_obj = await (prompt_2_study | model_2).ainvoke({"jobObjective": jobObjective})
        response_2 = response_2_obj.content
        return {"ability": response_1_text, "study": response_2, "lorem": lorem, "total_score": total_score}
    
async def talentedType_pipeline(resume) :
    response_3_obj = await (prompt_3 | model_3).ainvoke({"resume": resume})
    response_3 = response_3_obj.content
    return { "talentedType" : response_3 }

async def similarity_pipeline(lorem, preferred) :
    response_4_obj = await (prompt_4 | model_4).ainvoke({"lorem": lorem, "preferred": preferred})
    response_4 = response_4_obj.content
    return { "similarity" : response_4 }

# âœ… FastAPI ì„¤ì •
app = FastAPI()

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ (ì™¸ë¶€ ìš”ì²­ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í•„ìš”ì— ë”°ë¼ íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš© ê°€ëŠ¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ ëª¨ë¸ ì •ì˜
class ResumeRequest(BaseModel):
    lorem: str
    jobObjective: str
    
class TalentedTypeRequest(BaseModel):
    resume: str
    
class SimilarityRequest(BaseModel):
    lorem: str
    preferred: str

@app.post("/user/validate_resume")
async def validate_resume(request: ResumeRequest):
    print("ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì—°ê²°ëìŠµë‹ˆë‹¤.")
    lorem = request.lorem
    jobObjective = request.jobObjective 
    return await process_pipeline(lorem, jobObjective)

@app.post("/user/talentedType")
async def talentedType(request: TalentedTypeRequest):
    resume = request.resume
    return await talentedType_pipeline(resume)

@app.post("/employer/similarity")
async def similarity(request: SimilarityRequest):
    lorem = request.lorem
    preferred = request.preferred
    return await similarity_pipeline(lorem, preferred)

if __name__ == "__main__":
    uvicorn.run("main:app", host=f"{host}", port=port, reload=True)
